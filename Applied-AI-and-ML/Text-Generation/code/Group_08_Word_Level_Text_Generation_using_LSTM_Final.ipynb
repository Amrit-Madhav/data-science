{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Group-08:Word Level Text Generation using LSTM-Final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taoQZ2rebAD1",
        "outputId": "5512544f-6f93-4807-e7a1-420b1ed498ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### tensorflow = 2.3 and keras = 2.4 is needed for this code to execute."
      ],
      "metadata": {
        "id": "-dAC5UEbHiXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install q keras==2.4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 992
        },
        "id": "8iouq9w-AE3x",
        "outputId": "487ef44a-7e0b-4a00-f36f-7c81b4414a7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: q in /usr/local/lib/python3.7/dist-packages (2.6)\n",
            "Collecting keras==2.4\n",
            "  Using cached Keras-2.4.0-py2.py3-none-any.whl (170 kB)\n",
            "Requirement already satisfied: tensorflow>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.4) (2.3.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.4) (3.13)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.4) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.4) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.4) (2.10.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4) (3.17.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4) (0.37.1)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4) (0.3.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4) (1.46.3)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4) (1.1.2)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4) (2.9.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4) (2.3.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4) (1.6.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4) (1.14.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.2.0->keras==2.4) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.2.0->keras==2.4) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.2.0->keras==2.4) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.2.0->keras==2.4) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.2.0->keras==2.4) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.2.0->keras==2.4) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.2.0->keras==2.4) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.2.0->keras==2.4) (3.3.7)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=2.2.0->keras==2.4) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=2.2.0->keras==2.4) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=2.2.0->keras==2.4) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=2.2.0->keras==2.4) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=2.2.0->keras==2.4) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=2.2.0->keras==2.4) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=2.2.0->keras==2.4) (4.1.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=2.2.0->keras==2.4) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=2.2.0->keras==2.4) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=2.2.0->keras==2.4) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=2.2.0->keras==2.4) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=2.2.0->keras==2.4) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=2.2.0->keras==2.4) (3.2.0)\n",
            "Installing collected packages: keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: Keras 1.2.2\n",
            "    Uninstalling Keras-1.2.2:\n",
            "      Successfully uninstalled Keras-1.2.2\n",
            "Successfully installed keras-2.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w45-qiDACScH",
        "outputId": "547bdf49-cc4d-4e73-b780-0f5e15d469ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow==2.3 in /usr/local/lib/python3.7/dist-packages (2.3.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3) (2.10.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3) (1.6.3)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3) (1.18.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3) (3.3.0)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3) (2.9.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3) (2.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3) (1.14.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3) (0.2.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3) (1.46.3)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3) (1.1.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3) (0.37.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3) (3.17.3)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3) (0.3.3)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3) (1.4.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3) (1.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3) (1.1.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3) (0.4.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3) (57.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3) (2.23.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3) (3.3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3) (4.11.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf. __version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02CdO5-Y7FYE",
        "outputId": "4845c7a2-7065-41e5-fdd8-f6710e4832b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "\n",
        "keras.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IcsqF6vuDL7Z",
        "outputId": "cf915d70-f98d-4c8d-c1e7-c0b5f09a4e6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.4.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preparation\n",
        "We will start by preparing the data for modeling.\n",
        "Here we are developing a model of the text that we can then use to generate new sequences of text.\n",
        "\n",
        "The language model will be statistical and will predict the probability of each word given an input sequence of text. The predicted word will be fed in as input to in turn generate the next word."
      ],
      "metadata": {
        "id": "2bgoRk-CbMmL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load Text:\n",
        "function to load the entire text file into memory and return it. The function is called load_doc() "
      ],
      "metadata": {
        "id": "FjgvVqCvcBbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text"
      ],
      "metadata": {
        "id": "59nbHwpsbNb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load document\n",
        "in_filename = \"/content/drive/My Drive/Colab Notebooks/NLP/republic_clean.txt\"\n",
        "doc = load_doc(in_filename)\n",
        "print(doc[:200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPKZza3PcTfE",
        "outputId": "bc2cd4ff-1a4b-4fc3-91c9-d9a81fa9255c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿INTRODUCTION AND ANALYSIS.\n",
            "\n",
            "The Republic of Plato is the longest of his works with the exception\n",
            "of the Laws, and is certainly the greatest of them. There are nearer\n",
            "approaches to modern metaphysics \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clean Text:\n",
        "Replace ‘–‘ with a white space so we can split words better.\n",
        "Split words based on white space.\n",
        "\n",
        "Remove all punctuation from words to reduce the vocabulary size (e.g. ‘What?’ becomes ‘What’).\n",
        "\n",
        "Remove all words that are not alphabetic to remove standalone punctuation tokens.\n",
        "\n",
        "Normalize all words to lowercase to reduce the vocabulary size."
      ],
      "metadata": {
        "id": "-NrgATB_dQ6W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "clean_doc() that takes document as an argument and returns an array of clean tokens."
      ],
      "metadata": {
        "id": "bADF2husoTfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        " \n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\t# replace '--' with a space ' '\n",
        "\tdoc = doc.replace('--', ' ')\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# remove punctuation from each token\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\ttokens = [w.translate(table) for w in tokens]\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\t# make lower case\n",
        "\ttokens = [word.lower() for word in tokens]\n",
        "\treturn tokens"
      ],
      "metadata": {
        "id": "mOJ7cglFcW_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "cleaning operation on our loaded document"
      ],
      "metadata": {
        "id": "2OAknqaTogUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clean document\n",
        "tokens = clean_doc(doc)\n",
        "print(tokens[:200])\n",
        "print('Total Tokens: %d' % len(tokens))\n",
        "print('Unique Tokens: %d' % len(set(tokens)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSDp9NYGoaVP",
        "outputId": "183af84f-6350-4bba-e186-f2839eb40449"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['and', 'analysis', 'the', 'republic', 'of', 'plato', 'is', 'the', 'longest', 'of', 'his', 'works', 'with', 'the', 'exception', 'of', 'the', 'laws', 'and', 'is', 'certainly', 'the', 'greatest', 'of', 'them', 'there', 'are', 'nearer', 'approaches', 'to', 'modern', 'metaphysics', 'in', 'the', 'philebus', 'and', 'in', 'the', 'sophist', 'the', 'politicus', 'or', 'statesman', 'is', 'more', 'ideal', 'the', 'form', 'and', 'institutions', 'of', 'the', 'state', 'are', 'more', 'clearly', 'drawn', 'out', 'in', 'the', 'laws', 'as', 'works', 'of', 'art', 'the', 'symposium', 'and', 'the', 'protagoras', 'are', 'of', 'higher', 'excellence', 'but', 'no', 'other', 'dialogue', 'of', 'plato', 'has', 'the', 'same', 'largeness', 'of', 'view', 'and', 'the', 'same', 'perfection', 'of', 'style', 'no', 'other', 'shows', 'an', 'equal', 'knowledge', 'of', 'the', 'world', 'or', 'contains', 'more', 'of', 'those', 'thoughts', 'which', 'are', 'new', 'as', 'well', 'as', 'old', 'and', 'not', 'of', 'one', 'age', 'only', 'but', 'of', 'all', 'nowhere', 'in', 'plato', 'is', 'there', 'a', 'deeper', 'irony', 'or', 'a', 'greater', 'wealth', 'of', 'humour', 'or', 'imagery', 'or', 'more', 'dramatic', 'power', 'nor', 'in', 'any', 'other', 'of', 'his', 'writings', 'is', 'the', 'attempt', 'made', 'to', 'interweave', 'life', 'and', 'speculation', 'or', 'to', 'connect', 'politics', 'with', 'philosophy', 'the', 'republic', 'is', 'the', 'centre', 'around', 'which', 'the', 'other', 'dialogues', 'may', 'be', 'grouped', 'here', 'philosophy', 'reaches', 'the', 'highest', 'point', 'cp', 'especially', 'in', 'books', 'v', 'vi', 'vii', 'to', 'which', 'ancient', 'thinkers', 'ever', 'attained', 'plato', 'among', 'the']\n",
            "Total Tokens: 216690\n",
            "Unique Tokens: 10436\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save Clean Text\n",
        "we have organized the long list of tokens into sequences of 50 input words and 1 output word.\n",
        "\n",
        "That is, sequences of 51 words.\n",
        "\n",
        "We did this by iterating over the list of tokens from token 51 onwards and taking the prior 50 tokens as a sequence, then repeating this process to the end of the list of tokens.\n",
        "\n",
        "We have transform the tokens into space-separated strings for later storage in a file.\n",
        "\n",
        "The code to split the list of clean tokens into sequences with a length of 51 tokens is listed below."
      ],
      "metadata": {
        "id": "m3mW3_3ao4Wa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# organize into sequences of tokens\n",
        "length = 51 + 1\n",
        "sequences = list()\n",
        "\n",
        "for i in range(length, len(tokens)):\n",
        "\t# select sequence of tokens\n",
        "\tseq = tokens[i-length:i]\n",
        "\t# convert into a line\n",
        "\tline = ' '.join(seq)\n",
        "\t# store\n",
        "\tsequences.append(line)\n",
        "print('Total Sequences: %d' % len(sequences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfIyKqdXolzA",
        "outputId": "66a6cc2e-dc4d-42f7-c9c5-7883578234df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Sequences: 216638\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Printing statistics on the list, we can see that we will have exactly 216638 training patterns to fit our model."
      ],
      "metadata": {
        "id": "_Tr7b2vzpXBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(sequences[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrCkLmQLpJJG",
        "outputId": "249d2258-c5d6-4238-8f2f-061846fab14c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "and analysis the republic of plato is the longest of his works with the exception of the laws and is certainly the greatest of them there are nearer approaches to modern metaphysics in the philebus and in the sophist the politicus or statesman is more ideal the form and institutions of the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we have to save the sequences to a new file for later loading.\n",
        "\n",
        "We have defined a new function for saving lines of text to a file. This new function is called save_doc() .  It takes as input a list of lines and a filename. The lines are written, one per line, in ASCII format."
      ],
      "metadata": {
        "id": "C0ErqJsEpyxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save tokens to file, one dialog per line\n",
        "def save_doc(lines, filename):\n",
        "\tdata = '\\n'.join(lines)\n",
        "\tfile = open(filename, 'w')\n",
        "\tfile.write(data)\n",
        "\tfile.close()"
      ],
      "metadata": {
        "id": "cZHQWgWppYAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can call this function and save our training sequences to the file ‘republic_sequences.txt‘."
      ],
      "metadata": {
        "id": "4fnYzJS9p8QU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save sequences to file\n",
        "out_filename = 'republic_sequences.txt'\n",
        "save_doc(sequences, out_filename)"
      ],
      "metadata": {
        "id": "NPfYYisVp3RA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Language Model\n",
        "We can now train a statistical language model from the prepared data.\n",
        "\n",
        "The model we will train is a neural language model. It has a few unique characteristics:\n",
        "\n",
        "It uses a distributed representation for words so that different words with similar meanings will have a similar representation.\n",
        "It learns the representation at the same time as learning the model.\n",
        "It learns to predict the probability for the next word using the context of the last 100 words.\n",
        "Specifically, we have used an Embedding Layer to learn the representation of words, and a Long Short-Term Memory (LSTM) recurrent neural network to learn to predict words based on their context.\n",
        "\n",
        "#### Load training data."
      ],
      "metadata": {
        "id": "SvsQkWE9qVTZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Load Sequences**\n",
        "We can load our training data using the load_doc() function we developed in the previous section.\n",
        "\n",
        "Once loaded, we can split the data into separate training sequences by splitting based on new lines.\n",
        "\n",
        "The snippet below will load the ‘republic_sequences.txt‘ data file from the current working directory."
      ],
      "metadata": {
        "id": "JpfIrG_Jqj9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        " \n",
        "# load\n",
        "in_filename = 'republic_sequences.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')"
      ],
      "metadata": {
        "id": "PJ_2Ir03qBAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(lines[0:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEliZKd7oPdA",
        "outputId": "baac3863-5410-4d4c-97df-ef74f7fb8ba2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['and analysis the republic of plato is the longest of his works with the exception of the laws and is certainly the greatest of them there are nearer approaches to modern metaphysics in the philebus and in the sophist the politicus or statesman is more ideal the form and institutions of the', 'analysis the republic of plato is the longest of his works with the exception of the laws and is certainly the greatest of them there are nearer approaches to modern metaphysics in the philebus and in the sophist the politicus or statesman is more ideal the form and institutions of the state', 'the republic of plato is the longest of his works with the exception of the laws and is certainly the greatest of them there are nearer approaches to modern metaphysics in the philebus and in the sophist the politicus or statesman is more ideal the form and institutions of the state are', 'republic of plato is the longest of his works with the exception of the laws and is certainly the greatest of them there are nearer approaches to modern metaphysics in the philebus and in the sophist the politicus or statesman is more ideal the form and institutions of the state are more', 'of plato is the longest of his works with the exception of the laws and is certainly the greatest of them there are nearer approaches to modern metaphysics in the philebus and in the sophist the politicus or statesman is more ideal the form and institutions of the state are more clearly', 'plato is the longest of his works with the exception of the laws and is certainly the greatest of them there are nearer approaches to modern metaphysics in the philebus and in the sophist the politicus or statesman is more ideal the form and institutions of the state are more clearly drawn', 'is the longest of his works with the exception of the laws and is certainly the greatest of them there are nearer approaches to modern metaphysics in the philebus and in the sophist the politicus or statesman is more ideal the form and institutions of the state are more clearly drawn out', 'the longest of his works with the exception of the laws and is certainly the greatest of them there are nearer approaches to modern metaphysics in the philebus and in the sophist the politicus or statesman is more ideal the form and institutions of the state are more clearly drawn out in', 'longest of his works with the exception of the laws and is certainly the greatest of them there are nearer approaches to modern metaphysics in the philebus and in the sophist the politicus or statesman is more ideal the form and institutions of the state are more clearly drawn out in the', 'of his works with the exception of the laws and is certainly the greatest of them there are nearer approaches to modern metaphysics in the philebus and in the sophist the politicus or statesman is more ideal the form and institutions of the state are more clearly drawn out in the laws', 'his works with the exception of the laws and is certainly the greatest of them there are nearer approaches to modern metaphysics in the philebus and in the sophist the politicus or statesman is more ideal the form and institutions of the state are more clearly drawn out in the laws as', 'works with the exception of the laws and is certainly the greatest of them there are nearer approaches to modern metaphysics in the philebus and in the sophist the politicus or statesman is more ideal the form and institutions of the state are more clearly drawn out in the laws as works', 'with the exception of the laws and is certainly the greatest of them there are nearer approaches to modern metaphysics in the philebus and in the sophist the politicus or statesman is more ideal the form and institutions of the state are more clearly drawn out in the laws as works of', 'the exception of the laws and is certainly the greatest of them there are nearer approaches to modern metaphysics in the philebus and in the sophist the politicus or statesman is more ideal the form and institutions of the state are more clearly drawn out in the laws as works of art', 'exception of the laws and is certainly the greatest of them there are nearer approaches to modern metaphysics in the philebus and in the sophist the politicus or statesman is more ideal the form and institutions of the state are more clearly drawn out in the laws as works of art the', 'of the laws and is certainly the greatest of them there are nearer approaches to modern metaphysics in the philebus and in the sophist the politicus or statesman is more ideal the form and institutions of the state are more clearly drawn out in the laws as works of art the symposium', 'the laws and is certainly the greatest of them there are nearer approaches to modern metaphysics in the philebus and in the sophist the politicus or statesman is more ideal the form and institutions of the state are more clearly drawn out in the laws as works of art the symposium and', 'laws and is certainly the greatest of them there are nearer approaches to modern metaphysics in the philebus and in the sophist the politicus or statesman is more ideal the form and institutions of the state are more clearly drawn out in the laws as works of art the symposium and the', 'and is certainly the greatest of them there are nearer approaches to modern metaphysics in the philebus and in the sophist the politicus or statesman is more ideal the form and institutions of the state are more clearly drawn out in the laws as works of art the symposium and the protagoras', 'is certainly the greatest of them there are nearer approaches to modern metaphysics in the philebus and in the sophist the politicus or statesman is more ideal the form and institutions of the state are more clearly drawn out in the laws as works of art the symposium and the protagoras are', 'certainly the greatest of them there are nearer approaches to modern metaphysics in the philebus and in the sophist the politicus or statesman is more ideal the form and institutions of the state are more clearly drawn out in the laws as works of art the symposium and the protagoras are of', 'the greatest of them there are nearer approaches to modern metaphysics in the philebus and in the sophist the politicus or statesman is more ideal the form and institutions of the state are more clearly drawn out in the laws as works of art the symposium and the protagoras are of higher', 'greatest of them there are nearer approaches to modern metaphysics in the philebus and in the sophist the politicus or statesman is more ideal the form and institutions of the state are more clearly drawn out in the laws as works of art the symposium and the protagoras are of higher excellence', 'of them there are nearer approaches to modern metaphysics in the philebus and in the sophist the politicus or statesman is more ideal the form and institutions of the state are more clearly drawn out in the laws as works of art the symposium and the protagoras are of higher excellence but', 'them there are nearer approaches to modern metaphysics in the philebus and in the sophist the politicus or statesman is more ideal the form and institutions of the state are more clearly drawn out in the laws as works of art the symposium and the protagoras are of higher excellence but no', 'there are nearer approaches to modern metaphysics in the philebus and in the sophist the politicus or statesman is more ideal the form and institutions of the state are more clearly drawn out in the laws as works of art the symposium and the protagoras are of higher excellence but no other', 'are nearer approaches to modern metaphysics in the philebus and in the sophist the politicus or statesman is more ideal the form and institutions of the state are more clearly drawn out in the laws as works of art the symposium and the protagoras are of higher excellence but no other dialogue', 'nearer approaches to modern metaphysics in the philebus and in the sophist the politicus or statesman is more ideal the form and institutions of the state are more clearly drawn out in the laws as works of art the symposium and the protagoras are of higher excellence but no other dialogue of', 'approaches to modern metaphysics in the philebus and in the sophist the politicus or statesman is more ideal the form and institutions of the state are more clearly drawn out in the laws as works of art the symposium and the protagoras are of higher excellence but no other dialogue of plato', 'to modern metaphysics in the philebus and in the sophist the politicus or statesman is more ideal the form and institutions of the state are more clearly drawn out in the laws as works of art the symposium and the protagoras are of higher excellence but no other dialogue of plato has', 'modern metaphysics in the philebus and in the sophist the politicus or statesman is more ideal the form and institutions of the state are more clearly drawn out in the laws as works of art the symposium and the protagoras are of higher excellence but no other dialogue of plato has the', 'metaphysics in the philebus and in the sophist the politicus or statesman is more ideal the form and institutions of the state are more clearly drawn out in the laws as works of art the symposium and the protagoras are of higher excellence but no other dialogue of plato has the same', 'in the philebus and in the sophist the politicus or statesman is more ideal the form and institutions of the state are more clearly drawn out in the laws as works of art the symposium and the protagoras are of higher excellence but no other dialogue of plato has the same largeness', 'the philebus and in the sophist the politicus or statesman is more ideal the form and institutions of the state are more clearly drawn out in the laws as works of art the symposium and the protagoras are of higher excellence but no other dialogue of plato has the same largeness of', 'philebus and in the sophist the politicus or statesman is more ideal the form and institutions of the state are more clearly drawn out in the laws as works of art the symposium and the protagoras are of higher excellence but no other dialogue of plato has the same largeness of view', 'and in the sophist the politicus or statesman is more ideal the form and institutions of the state are more clearly drawn out in the laws as works of art the symposium and the protagoras are of higher excellence but no other dialogue of plato has the same largeness of view and', 'in the sophist the politicus or statesman is more ideal the form and institutions of the state are more clearly drawn out in the laws as works of art the symposium and the protagoras are of higher excellence but no other dialogue of plato has the same largeness of view and the', 'the sophist the politicus or statesman is more ideal the form and institutions of the state are more clearly drawn out in the laws as works of art the symposium and the protagoras are of higher excellence but no other dialogue of plato has the same largeness of view and the same', 'sophist the politicus or statesman is more ideal the form and institutions of the state are more clearly drawn out in the laws as works of art the symposium and the protagoras are of higher excellence but no other dialogue of plato has the same largeness of view and the same perfection', 'the politicus or statesman is more ideal the form and institutions of the state are more clearly drawn out in the laws as works of art the symposium and the protagoras are of higher excellence but no other dialogue of plato has the same largeness of view and the same perfection of', 'politicus or statesman is more ideal the form and institutions of the state are more clearly drawn out in the laws as works of art the symposium and the protagoras are of higher excellence but no other dialogue of plato has the same largeness of view and the same perfection of style', 'or statesman is more ideal the form and institutions of the state are more clearly drawn out in the laws as works of art the symposium and the protagoras are of higher excellence but no other dialogue of plato has the same largeness of view and the same perfection of style no', 'statesman is more ideal the form and institutions of the state are more clearly drawn out in the laws as works of art the symposium and the protagoras are of higher excellence but no other dialogue of plato has the same largeness of view and the same perfection of style no other', 'is more ideal the form and institutions of the state are more clearly drawn out in the laws as works of art the symposium and the protagoras are of higher excellence but no other dialogue of plato has the same largeness of view and the same perfection of style no other shows', 'more ideal the form and institutions of the state are more clearly drawn out in the laws as works of art the symposium and the protagoras are of higher excellence but no other dialogue of plato has the same largeness of view and the same perfection of style no other shows an', 'ideal the form and institutions of the state are more clearly drawn out in the laws as works of art the symposium and the protagoras are of higher excellence but no other dialogue of plato has the same largeness of view and the same perfection of style no other shows an equal', 'the form and institutions of the state are more clearly drawn out in the laws as works of art the symposium and the protagoras are of higher excellence but no other dialogue of plato has the same largeness of view and the same perfection of style no other shows an equal knowledge', 'form and institutions of the state are more clearly drawn out in the laws as works of art the symposium and the protagoras are of higher excellence but no other dialogue of plato has the same largeness of view and the same perfection of style no other shows an equal knowledge of', 'and institutions of the state are more clearly drawn out in the laws as works of art the symposium and the protagoras are of higher excellence but no other dialogue of plato has the same largeness of view and the same perfection of style no other shows an equal knowledge of the', 'institutions of the state are more clearly drawn out in the laws as works of art the symposium and the protagoras are of higher excellence but no other dialogue of plato has the same largeness of view and the same perfection of style no other shows an equal knowledge of the world', 'of the state are more clearly drawn out in the laws as works of art the symposium and the protagoras are of higher excellence but no other dialogue of plato has the same largeness of view and the same perfection of style no other shows an equal knowledge of the world or', 'the state are more clearly drawn out in the laws as works of art the symposium and the protagoras are of higher excellence but no other dialogue of plato has the same largeness of view and the same perfection of style no other shows an equal knowledge of the world or contains', 'state are more clearly drawn out in the laws as works of art the symposium and the protagoras are of higher excellence but no other dialogue of plato has the same largeness of view and the same perfection of style no other shows an equal knowledge of the world or contains more', 'are more clearly drawn out in the laws as works of art the symposium and the protagoras are of higher excellence but no other dialogue of plato has the same largeness of view and the same perfection of style no other shows an equal knowledge of the world or contains more of', 'more clearly drawn out in the laws as works of art the symposium and the protagoras are of higher excellence but no other dialogue of plato has the same largeness of view and the same perfection of style no other shows an equal knowledge of the world or contains more of those', 'clearly drawn out in the laws as works of art the symposium and the protagoras are of higher excellence but no other dialogue of plato has the same largeness of view and the same perfection of style no other shows an equal knowledge of the world or contains more of those thoughts', 'drawn out in the laws as works of art the symposium and the protagoras are of higher excellence but no other dialogue of plato has the same largeness of view and the same perfection of style no other shows an equal knowledge of the world or contains more of those thoughts which', 'out in the laws as works of art the symposium and the protagoras are of higher excellence but no other dialogue of plato has the same largeness of view and the same perfection of style no other shows an equal knowledge of the world or contains more of those thoughts which are', 'in the laws as works of art the symposium and the protagoras are of higher excellence but no other dialogue of plato has the same largeness of view and the same perfection of style no other shows an equal knowledge of the world or contains more of those thoughts which are new', 'the laws as works of art the symposium and the protagoras are of higher excellence but no other dialogue of plato has the same largeness of view and the same perfection of style no other shows an equal knowledge of the world or contains more of those thoughts which are new as', 'laws as works of art the symposium and the protagoras are of higher excellence but no other dialogue of plato has the same largeness of view and the same perfection of style no other shows an equal knowledge of the world or contains more of those thoughts which are new as well', 'as works of art the symposium and the protagoras are of higher excellence but no other dialogue of plato has the same largeness of view and the same perfection of style no other shows an equal knowledge of the world or contains more of those thoughts which are new as well as', 'works of art the symposium and the protagoras are of higher excellence but no other dialogue of plato has the same largeness of view and the same perfection of style no other shows an equal knowledge of the world or contains more of those thoughts which are new as well as old', 'of art the symposium and the protagoras are of higher excellence but no other dialogue of plato has the same largeness of view and the same perfection of style no other shows an equal knowledge of the world or contains more of those thoughts which are new as well as old and', 'art the symposium and the protagoras are of higher excellence but no other dialogue of plato has the same largeness of view and the same perfection of style no other shows an equal knowledge of the world or contains more of those thoughts which are new as well as old and not', 'the symposium and the protagoras are of higher excellence but no other dialogue of plato has the same largeness of view and the same perfection of style no other shows an equal knowledge of the world or contains more of those thoughts which are new as well as old and not of', 'symposium and the protagoras are of higher excellence but no other dialogue of plato has the same largeness of view and the same perfection of style no other shows an equal knowledge of the world or contains more of those thoughts which are new as well as old and not of one', 'and the protagoras are of higher excellence but no other dialogue of plato has the same largeness of view and the same perfection of style no other shows an equal knowledge of the world or contains more of those thoughts which are new as well as old and not of one age', 'the protagoras are of higher excellence but no other dialogue of plato has the same largeness of view and the same perfection of style no other shows an equal knowledge of the world or contains more of those thoughts which are new as well as old and not of one age only', 'protagoras are of higher excellence but no other dialogue of plato has the same largeness of view and the same perfection of style no other shows an equal knowledge of the world or contains more of those thoughts which are new as well as old and not of one age only but', 'are of higher excellence but no other dialogue of plato has the same largeness of view and the same perfection of style no other shows an equal knowledge of the world or contains more of those thoughts which are new as well as old and not of one age only but of', 'of higher excellence but no other dialogue of plato has the same largeness of view and the same perfection of style no other shows an equal knowledge of the world or contains more of those thoughts which are new as well as old and not of one age only but of all', 'higher excellence but no other dialogue of plato has the same largeness of view and the same perfection of style no other shows an equal knowledge of the world or contains more of those thoughts which are new as well as old and not of one age only but of all nowhere', 'excellence but no other dialogue of plato has the same largeness of view and the same perfection of style no other shows an equal knowledge of the world or contains more of those thoughts which are new as well as old and not of one age only but of all nowhere in', 'but no other dialogue of plato has the same largeness of view and the same perfection of style no other shows an equal knowledge of the world or contains more of those thoughts which are new as well as old and not of one age only but of all nowhere in plato', 'no other dialogue of plato has the same largeness of view and the same perfection of style no other shows an equal knowledge of the world or contains more of those thoughts which are new as well as old and not of one age only but of all nowhere in plato is', 'other dialogue of plato has the same largeness of view and the same perfection of style no other shows an equal knowledge of the world or contains more of those thoughts which are new as well as old and not of one age only but of all nowhere in plato is there', 'dialogue of plato has the same largeness of view and the same perfection of style no other shows an equal knowledge of the world or contains more of those thoughts which are new as well as old and not of one age only but of all nowhere in plato is there a', 'of plato has the same largeness of view and the same perfection of style no other shows an equal knowledge of the world or contains more of those thoughts which are new as well as old and not of one age only but of all nowhere in plato is there a deeper', 'plato has the same largeness of view and the same perfection of style no other shows an equal knowledge of the world or contains more of those thoughts which are new as well as old and not of one age only but of all nowhere in plato is there a deeper irony', 'has the same largeness of view and the same perfection of style no other shows an equal knowledge of the world or contains more of those thoughts which are new as well as old and not of one age only but of all nowhere in plato is there a deeper irony or', 'the same largeness of view and the same perfection of style no other shows an equal knowledge of the world or contains more of those thoughts which are new as well as old and not of one age only but of all nowhere in plato is there a deeper irony or a', 'same largeness of view and the same perfection of style no other shows an equal knowledge of the world or contains more of those thoughts which are new as well as old and not of one age only but of all nowhere in plato is there a deeper irony or a greater', 'largeness of view and the same perfection of style no other shows an equal knowledge of the world or contains more of those thoughts which are new as well as old and not of one age only but of all nowhere in plato is there a deeper irony or a greater wealth', 'of view and the same perfection of style no other shows an equal knowledge of the world or contains more of those thoughts which are new as well as old and not of one age only but of all nowhere in plato is there a deeper irony or a greater wealth of', 'view and the same perfection of style no other shows an equal knowledge of the world or contains more of those thoughts which are new as well as old and not of one age only but of all nowhere in plato is there a deeper irony or a greater wealth of humour', 'and the same perfection of style no other shows an equal knowledge of the world or contains more of those thoughts which are new as well as old and not of one age only but of all nowhere in plato is there a deeper irony or a greater wealth of humour or', 'the same perfection of style no other shows an equal knowledge of the world or contains more of those thoughts which are new as well as old and not of one age only but of all nowhere in plato is there a deeper irony or a greater wealth of humour or imagery', 'same perfection of style no other shows an equal knowledge of the world or contains more of those thoughts which are new as well as old and not of one age only but of all nowhere in plato is there a deeper irony or a greater wealth of humour or imagery or', 'perfection of style no other shows an equal knowledge of the world or contains more of those thoughts which are new as well as old and not of one age only but of all nowhere in plato is there a deeper irony or a greater wealth of humour or imagery or more', 'of style no other shows an equal knowledge of the world or contains more of those thoughts which are new as well as old and not of one age only but of all nowhere in plato is there a deeper irony or a greater wealth of humour or imagery or more dramatic', 'style no other shows an equal knowledge of the world or contains more of those thoughts which are new as well as old and not of one age only but of all nowhere in plato is there a deeper irony or a greater wealth of humour or imagery or more dramatic power', 'no other shows an equal knowledge of the world or contains more of those thoughts which are new as well as old and not of one age only but of all nowhere in plato is there a deeper irony or a greater wealth of humour or imagery or more dramatic power nor', 'other shows an equal knowledge of the world or contains more of those thoughts which are new as well as old and not of one age only but of all nowhere in plato is there a deeper irony or a greater wealth of humour or imagery or more dramatic power nor in', 'shows an equal knowledge of the world or contains more of those thoughts which are new as well as old and not of one age only but of all nowhere in plato is there a deeper irony or a greater wealth of humour or imagery or more dramatic power nor in any', 'an equal knowledge of the world or contains more of those thoughts which are new as well as old and not of one age only but of all nowhere in plato is there a deeper irony or a greater wealth of humour or imagery or more dramatic power nor in any other', 'equal knowledge of the world or contains more of those thoughts which are new as well as old and not of one age only but of all nowhere in plato is there a deeper irony or a greater wealth of humour or imagery or more dramatic power nor in any other of', 'knowledge of the world or contains more of those thoughts which are new as well as old and not of one age only but of all nowhere in plato is there a deeper irony or a greater wealth of humour or imagery or more dramatic power nor in any other of his', 'of the world or contains more of those thoughts which are new as well as old and not of one age only but of all nowhere in plato is there a deeper irony or a greater wealth of humour or imagery or more dramatic power nor in any other of his writings', 'the world or contains more of those thoughts which are new as well as old and not of one age only but of all nowhere in plato is there a deeper irony or a greater wealth of humour or imagery or more dramatic power nor in any other of his writings is']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Encode Sequences**\n",
        "The word embedding layer expects input sequences to be comprised of integers.\n",
        "\n",
        "We can map each word in our vocabulary to a unique integer and encode our input sequences. Later, when we make predictions, we can convert the prediction to numbers and look up their associated words in the same mapping.\n",
        "\n",
        "To do this encoding, we have used the Tokenizer class in the Keras API.\n",
        "\n",
        "First, the Tokenizer must be trained on the entire training dataset, which means it finds all of the unique words in the data and assigns each a unique integer.\n",
        "\n",
        "We can then use the fit Tokenizer to encode all of the training sequences, converting each sequence from a list of words to a list of integers."
      ],
      "metadata": {
        "id": "WPHGufCBq6I3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRaM2rSYrjlu",
        "outputId": "2e1648c0-73d2-403d-9287-9c9c93734a0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# integer encode sequences of words\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "sequences = tokenizer.texts_to_sequences(lines)"
      ],
      "metadata": {
        "id": "1F55X_t5qrf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0uAFUC5rGEM",
        "outputId": "f90662de-efc4-4493-cc4a-8ee1d09d17aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can access the mapping of words to integers as a dictionary attribute called word_index on the Tokenizer object.\n",
        "\n",
        "We need to know the size of the vocabulary for defining the embedding layer later. We can determine the vocabulary by calculating the size of the mapping dictionary.\n",
        "\n",
        "Words are assigned values from 1 to the total number of words ( 10436). The Embedding layer needs to allocate a vector representation for each word in this vocabulary from index 1 to the largest index and because indexing of arrays is zero-offset, the index of the word at the end of the vocabulary is 10436; that means the array must be 10436 + 1 in length.\n",
        "\n",
        "Therefore, when specifying the vocabulary size to the Embedding layer, we specify it as 1 larger than the actual vocabulary.Here euqals (**10437**)"
      ],
      "metadata": {
        "id": "Zw2l2H_htNc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "id": "kKCbWB92rAbl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56c88a0a-238c-4ac0-946b-bfc0c00372c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10437\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Sequence Inputs and Output**\n",
        "**Now that we have encoded the input sequences, we need to separate them into input (X) and output (y) elements.**\n",
        "\n",
        "We can do this with array slicing.\n",
        "\n",
        "After separating, **we need to one hot encode the output word. This means converting it from an integer to a vector of 0 values, one for each word in the vocabulary, with a 1 to indicate the specific word at the index of the words integer value.**\n",
        "\n",
        "This is so that the model learns to predict the probability distribution for the next word and the ground truth from which to learn from is 0 for all words except the actual word that comes next.\n",
        "\n",
        "Keras provides the **to_categorical()** that can be used to one hot encode the output words for each input-output sequence pair.\n",
        "\n",
        "**Finally, we need to specify to the Embedding layer** how long input sequences are. We know that there are 50 words because we designed the model, **but a good generic way to specify that is to use the second dimension (number of columns)** **of the input data’s shape.** That way, if you change the length of sequences when preparing data, you do not need to change this data loading code; it is generic."
      ],
      "metadata": {
        "id": "9MiXo4M8tjKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# separate into input and output\n",
        "from array import array\n",
        "from numpy import array\n",
        "from keras.utils.np_utils import to_categorical\n",
        "sequences = array(sequences)\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "seq_length = X.shape[1]"
      ],
      "metadata": {
        "id": "RX0n35nmtQ-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59wUKcZmq_A3",
        "outputId": "d22e622a-a930-4611-d0be-f142ceaa4d81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Fit Model**\n",
        "We can now define and fit our language model on the training data.\n",
        "\n",
        "The learned embedding needs to know the size of the vocabulary and the length of input sequences.\n",
        "\n",
        "We have used a two LSTM hidden layers with 100 memory cells each. More memory cells and a deeper network may achieve better results.\n",
        "\n",
        "A dense fully connected layer with 100 neurons connects to the LSTM hidden layers to interpret the features extracted from the sequence. \n",
        "\n",
        "The output layer predicts the next word as a single vector the size of the vocabulary with a probability for each word in the vocabulary.\n",
        "\n",
        "A softmax activation function is used to ensure the outputs have the characteristics of normalized probabilities."
      ],
      "metadata": {
        "id": "xpPiRzVe3NJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Larger LSTM Network to Generate Text for Alice in Wonderland\n",
        "from numpy import array\n",
        "from pickle import dump\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "#from keras.utils import to_categorical\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "# define model\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "j9i-dEBKtteD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6a4685e-4aa5-475a-dadb-85305e291556"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 51, 50)            521850    \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 51, 100)           60400     \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 10437)             1054137   \n",
            "=================================================================\n",
            "Total params: 1,726,887\n",
            "Trainable params: 1,726,887\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " the model is compiled specifying the categorical cross entropy loss needed to fit the model.\n",
        "\n",
        "the model is learning a multi-class classification and this is the suitable loss function for this type of problem. The efficient Adam optimizer is being used and accuracy is evaluated of the model.\n",
        "\n",
        "Adam is an optimization algorithm that can be used instead of the classical stochastic gradient descent procedure to update network weights.\n",
        "\n",
        "Adam benefits: easy to implement\n",
        "             : Well suited for problems that are large in terms of data and/or parameters.\n",
        "             : uses features of Adaptive Gradient Algorithm (AdaGrad)  and \n",
        "             Root Mean Square Propagation (RMSProp)."
      ],
      "metadata": {
        "id": "eG-wz7yG4Nvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit model\n",
        "model.fit(X, y, batch_size=128, epochs=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJyp6b7y4cOb",
        "outputId": "b7af9ea8-73b8-4b86-9dc7-996eca05019c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1693/1693 [==============================] - 495s 292ms/step - loss: 6.1214 - accuracy: 0.1001\n",
            "Epoch 2/20\n",
            "1693/1693 [==============================] - 497s 294ms/step - loss: 5.6153 - accuracy: 0.1380\n",
            "Epoch 3/20\n",
            "1693/1693 [==============================] - 500s 295ms/step - loss: 5.4024 - accuracy: 0.1545\n",
            "Epoch 4/20\n",
            "1693/1693 [==============================] - 503s 297ms/step - loss: 5.2574 - accuracy: 0.1648\n",
            "Epoch 5/20\n",
            "1693/1693 [==============================] - 500s 296ms/step - loss: 5.1313 - accuracy: 0.1729\n",
            "Epoch 6/20\n",
            "1693/1693 [==============================] - 496s 293ms/step - loss: 5.0211 - accuracy: 0.1791\n",
            "Epoch 7/20\n",
            "1693/1693 [==============================] - 511s 302ms/step - loss: 4.9221 - accuracy: 0.1847\n",
            "Epoch 8/20\n",
            "1693/1693 [==============================] - 500s 295ms/step - loss: 4.8338 - accuracy: 0.1885\n",
            "Epoch 9/20\n",
            "1693/1693 [==============================] - 498s 294ms/step - loss: 4.7546 - accuracy: 0.1918\n",
            "Epoch 10/20\n",
            "1693/1693 [==============================] - 500s 295ms/step - loss: 4.6828 - accuracy: 0.1949\n",
            "Epoch 11/20\n",
            "1693/1693 [==============================] - 502s 296ms/step - loss: 4.6171 - accuracy: 0.1978\n",
            "Epoch 12/20\n",
            "1693/1693 [==============================] - 486s 287ms/step - loss: 4.5579 - accuracy: 0.1999\n",
            "Epoch 13/20\n",
            "1693/1693 [==============================] - 482s 284ms/step - loss: 4.5027 - accuracy: 0.2023\n",
            "Epoch 14/20\n",
            "1693/1693 [==============================] - 483s 286ms/step - loss: 4.4530 - accuracy: 0.2044\n",
            "Epoch 15/20\n",
            "1693/1693 [==============================] - 492s 291ms/step - loss: 4.4071 - accuracy: 0.2063\n",
            "Epoch 16/20\n",
            "1693/1693 [==============================] - 501s 296ms/step - loss: 4.3649 - accuracy: 0.2089\n",
            "Epoch 17/20\n",
            "1693/1693 [==============================] - 500s 295ms/step - loss: 4.3253 - accuracy: 0.2114\n",
            "Epoch 18/20\n",
            "1693/1693 [==============================] - 502s 296ms/step - loss: 4.2884 - accuracy: 0.2125\n",
            "Epoch 19/20\n",
            "1693/1693 [==============================] - 499s 295ms/step - loss: 4.2537 - accuracy: 0.2153\n",
            "Epoch 20/20\n",
            "1693/1693 [==============================] - 499s 295ms/step - loss: 4.2218 - accuracy: 0.2176\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f278a9c93d0>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "an accuracy of just over 50% of predicting the next word in the sequence, is always good. We are not aiming for 100% accuracy (e.g. a model that memorized the text), but rather a model that captures the essence of the text.\n",
        "we can optimize by increasing hidden layers and more LSTM layer"
      ],
      "metadata": {
        "id": "1rS0X4f64_ZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save Model"
      ],
      "metadata": {
        "id": "xrX-sblX5EF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model to file\n",
        "model.save('model.h5')\n",
        "# save the tokenizer\n",
        "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "E4MNyYUV4fvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Use Language Model**\n",
        "\n",
        "Now that we have a trained language model, we can use it.\n",
        "\n",
        "In this case, we can use it to generate new sequences of text that have the same statistical properties as the source text."
      ],
      "metadata": {
        "id": "nN-2YZ5gfWwT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Data"
      ],
      "metadata": {
        "id": "FZyPuwxLfrcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        " \n",
        "# load cleaned text sequences\n",
        "in_filename = 'republic_sequences.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')"
      ],
      "metadata": {
        "id": "45Qe2Z5KfjxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need the text so that we can choose a source sequence as input to the model for generating a new sequence of text.\n",
        "\n",
        "The model will require 50 words as input.\n",
        "\n",
        "Later, we will need to specify the expected length of input. We can determine this from the input sequences by calculating the length of one line of the loaded data and subtracting 1 for the expected output word that is also on the same line."
      ],
      "metadata": {
        "id": "jJS9EEcCf97t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Model\n",
        "\n",
        "We can now load the model from file.\n",
        "\n",
        "Keras provides the load_model() function for loading the model"
      ],
      "metadata": {
        "id": "aYWZnQ7mgCSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the model\n",
        "from random import randint\n",
        "from pickle import load\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "model = load_model('model.h5')"
      ],
      "metadata": {
        "id": "Hsj-AkiUf-0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also load the tokenizer from file using the Pickle API."
      ],
      "metadata": {
        "id": "at8emN8RgWtH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the tokenizer\n",
        "tokenizer = load(open('tokenizer.pkl', 'rb'))"
      ],
      "metadata": {
        "id": "07DLAnX2gOTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Generate Text**\n",
        "The first step in generating text is preparing a seed input.\n",
        "\n",
        "We will select a random line of text from the input text for this purpose. Once selected, we will print it so that we have some idea of what was used."
      ],
      "metadata": {
        "id": "Dt10e0TTgff_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(lines))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0ZRjUwr4pCN",
        "outputId": "636c7117-cf5b-44dd-b716-c064b2333409"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "216638\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# select a seed text\n",
        "seed_text = lines[randint(0,len(lines))]\n",
        "print(seed_text + '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55uHqXOigi_-",
        "outputId": "3f137bc5-f8cc-4d9e-ea89-e2fe3419729d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "intemperate and worthless subjects even though they might have made large fortunes out of them as to the story of pindar that asclepius was slain by a thunderbolt for restoring a rich man to life that is a lie following our old rule we must say either that he did not take\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we can generate new words, one at a time.\n",
        "\n",
        "First, the seed text must be encoded to integers using the same tokenizer that we used when training the model."
      ],
      "metadata": {
        "id": "QUa6ZqySg91Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = tokenizer.texts_to_sequences([seed_text])[0]"
      ],
      "metadata": {
        "id": "qPtk-PRAhAr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encoded)\n",
        "print(len(encoded))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NvXWwJ14Ygf",
        "outputId": "6dae745e-5ec1-48ed-cd05-ea00a2de2678"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3409, 3, 3410, 433, 159, 240, 15, 168, 20, 133, 821, 2937, 106, 2, 25, 17, 4, 1, 1403, 2, 2234, 9, 1128, 50, 2938, 23, 7, 6859, 26, 3967, 7, 374, 54, 4, 76, 9, 5, 7, 457, 1060, 58, 198, 277, 21, 61, 71, 137, 9, 8, 278, 12, 149]\n",
            "52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model can predict the next word directly by calling model.predict_classes() that will return the index of the word with the highest probability."
      ],
      "metadata": {
        "id": "dlq7PsknhDm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# predict probabilities for each word\n",
        "import numpy as np\n",
        "from random import randint\n",
        "from pickle import load\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "yhat = model.predict_classes(encoded, verbose=0)\n",
        "#yhat = np.argmax(model.predict_class(encoded))\n",
        "#yhat = (model.predict(encoded) > 0.5).astype(\"int32\")\n",
        "#yhat = model.predict_proba(encoded, verbose=0)\n",
        "#predict_x=model.predict(encoded)\n",
        "#predict_x = np.argmax(model.predict(encoded), axis=-1)\n",
        "#classes_x=np.argmax(predict_x,axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRoGY3SZhH60",
        "outputId": "53d951b7-d21e-4cc1-b679-ccff5c57072d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From <ipython-input-32-fc4912e456af>:8: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
            "Instructions for updating:\n",
            "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 51) for input Tensor(\"embedding_input_1:0\", shape=(None, 51), dtype=float32), but it was called on an input with incompatible shape (None, 1).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then look up the index in the Tokenizers mapping to get the associated word."
      ],
      "metadata": {
        "id": "U8dPtcvJhMbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out_word = ''\n",
        "for word, index in tokenizer.word_index.items():\n",
        "\tif index == yhat:\n",
        "\t\tout_word = word\n",
        "\t\tbreak"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "5NLp9ANXhQiY",
        "outputId": "e9b933e9-06a9-4825-cea0-90e8df21b8a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-4efeb1fd01af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mout_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0myhat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m                 \u001b[0mout_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importantly, the input sequence is going to get too long. We can truncate it to the desired length after the input sequence has been encoded to integers. Keras provides the pad_sequences() function that we can use to perform this truncation."
      ],
      "metadata": {
        "id": "DgrMpkbIhiNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')"
      ],
      "metadata": {
        "id": "HQV5V1X-hjH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can wrap all of this into a function called generate_seq() that takes as input the model, the tokenizer, input sequence length, the seed text, and the number of words to generate. It then returns a sequence of words generated by the model."
      ],
      "metadata": {
        "id": "qijQh__KhmUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from random import randint\n",
        "from pickle import load\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        " \n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        " \n",
        "# generate a sequence from a language model\n",
        "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
        "\tresult = list()\n",
        "\tin_text = seed_text\n",
        "\t# generate a fixed number of words\n",
        "\tfor _ in range(n_words):\n",
        "\t\t# encode the text as integer\n",
        "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "\t\t# truncate sequences to a fixed length\n",
        "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "\t\t# predict probabilities for each word\n",
        "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
        "\t\t# map predicted word index to word\n",
        "\t\tout_word = ''\n",
        "\t\tfor word, index in tokenizer.word_index.items():\n",
        "\t\t\tif index == yhat:\n",
        "\t\t\t\tout_word = word\n",
        "\t\t\t\tbreak\n",
        "\t\t# append to input\n",
        "\t\tin_text += ' ' + out_word\n",
        "\t\tresult.append(out_word)\n",
        "\treturn ' '.join(result)\n",
        " \n",
        "# load cleaned text sequences\n",
        "in_filename = 'republic_sequences.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')\n",
        "seq_length = len(lines[0].split()) - 1\n",
        " \n",
        "# load the model\n",
        "model = load_model('model.h5')\n",
        " \n",
        "# load the tokenizer\n",
        "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
        " \n",
        "# select a seed text\n",
        "seed_text = lines[randint(0,len(lines))]\n",
        "print(\"text input : \" , seed_text + '\\n')\n",
        "seed_text = 'i am happy'\n",
        "print(\"text input : \" , seed_text + '\\n')\n",
        "\n",
        "# generate new text\n",
        "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
        "print(\"Text Generated console: \\n\\n\", generated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wY98VgXohrGr",
        "outputId": "d9b0004d-74b0-40fc-d1c5-796472fa3a8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text input :  abate until he have attained the knowledge of the true nature of every essence by a sympathetic and kindred power in the soul and by that power drawing near and mingling and becoming incorporate with very being having begotten mind and truth he will have knowledge and will live and grow truly\n",
            "\n",
            "text input :  i am happy\n",
            "\n",
            "Text Generated console: \n",
            "\n",
            " and the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the state of the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tAq9huPSFyi4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}