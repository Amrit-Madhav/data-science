{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Group8: 1.Char Level Text Generation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdVywgx451R1",
        "outputId": "01372016-012d-4075-e1f3-e8740945da85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Character Level Text Generation with an LSTM Model"
      ],
      "metadata": {
        "id": "2r8V_AHN56cG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Language Model can be trained to generate text character-by-character. In this scenario, each of the input and output tokens is a character. Moreover, Language Model outputs a conditional probability distribution over character set."
      ],
      "metadata": {
        "id": "iko2tehz6Y9N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TensorFlow Pipeline"
      ],
      "metadata": {
        "id": "qve9N3WF7WTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "import numpy as np\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import layers, Model\n",
        "import os\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "import string\n",
        "import re"
      ],
      "metadata": {
        "id": "79ch4BDG7gxi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_dataset(dataset,fileName):\n",
        "  path = os.path.join('./tfDatasets/', fileName)\n",
        "  tf.data.experimental.save(dataset, path)\n",
        "\n",
        "def load_dataset(fileName):\n",
        "  path = os.path.join(\"./tfDatasets/\", fileName)\n",
        "  new_dataset = tf.data.experimental.load(path,\n",
        "      tf.TensorSpec(shape=(), dtype=tf.string))\n",
        "  return new_dataset"
      ],
      "metadata": {
        "id": "CQ7eOf4FUV-1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "raw_data_ds = tf.data.TextLineDataset([\"/content/drive/My Drive/Colab Notebooks/NLP/republic_clean.txt\"])"
      ],
      "metadata": {
        "id": "oaDxFTNe6HWG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fG4yPv4RUByB",
        "outputId": "8308928b-eb50-4891-f803-285728b5b3d7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## converted into text\n",
        "text=\"\"\n",
        "for elem in raw_data_ds:\n",
        "   text=text+(elem.numpy().decode('utf-8'))"
      ],
      "metadata": {
        "id": "yqRhfHU17bRf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "chars = sorted(list(set(text)))\n",
        "print(\"Total disctinct chars:\", len(chars))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmQic5d-STq2",
        "outputId": "44f28751-bd32-47f4-e216-40f0728dc6ad"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total disctinct chars: 79\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cutting the text in semi-redundant sequences of maxlen characters\n",
        "maxlen = 20\n",
        "step = 2\n",
        "input_chars = []\n",
        "next_char = []"
      ],
      "metadata": {
        "id": "U99IIFeOVRQK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, len(text) - maxlen, step):\n",
        "    input_chars.append(text[i : i + maxlen])\n",
        "    next_char.append(text[i + maxlen])"
      ],
      "metadata": {
        "id": "Ootbli9dVUqD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of sequences:\", len(input_chars))\n",
        "print(\"input X  (input_chars)  --->   output y (next_char) \")\n",
        "\n",
        "for i in range(5):\n",
        "  print( input_chars[i],\"   --->  \", next_char[i])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThxTBHRXVXYb",
        "outputId": "4e5955e3-04ea-41bc-ad02-5eee8e71db11"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sequences: 585326\n",
            "input X  (input_chars)  --->   output y (next_char) \n",
            "ï»¿INTRODUCTION AND AN    --->   A\n",
            "NTRODUCTION AND ANAL    --->   Y\n",
            "RODUCTION AND ANALYS    --->   I\n",
            "DUCTION AND ANALYSIS    --->   .\n",
            "CTION AND ANALYSIS.T    --->   h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_ds_raw=tf.data.Dataset.from_tensor_slices(input_chars)\n",
        "y_train_ds_raw=tf.data.Dataset.from_tensor_slices(next_char)"
      ],
      "metadata": {
        "id": "Gg8_623nVau7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train_ds_raw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYGrWXg8iILx",
        "outputId": "1efe0528-0f7e-4c8e-aa3f-feeccdcf7d93"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_standardization(input_data):\n",
        "    lowercase     = tf.strings.lower(input_data)\n",
        "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
        "    stripped_num  = tf.strings.regex_replace(stripped_html, \"[\\d-]\", \" \")\n",
        "    stripped_punc  =tf.strings.regex_replace(stripped_num, \n",
        "                             \"[%s]\" % re.escape(string.punctuation), \"\")    \n",
        "    return stripped_punc\n",
        "\n",
        "def char_split(input_data):\n",
        "  return tf.strings.unicode_split(input_data, 'UTF-8')\n",
        "\n",
        "def word_split(input_data):\n",
        "  return tf.strings.split(input_data)"
      ],
      "metadata": {
        "id": "_kVirInRVdUu"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model constants.\n",
        "max_features = 96           # Number of distinct chars / words  \n",
        "embedding_dim = 16             # Embedding layer output dimension\n",
        "sequence_length = maxlen       # Input sequence size"
      ],
      "metadata": {
        "id": "3AwSbC5KVf10"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=max_features,\n",
        "    split=char_split, # word_split or char_split\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")"
      ],
      "metadata": {
        "id": "Z3R5Tm5XVjDa"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer.adapt(X_train_ds_raw.batch(batch_size))"
      ],
      "metadata": {
        "id": "5YxDWHoeWEsm"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The size of the vocabulary (number of distinct characters): \", len(vectorize_layer.get_vocabulary()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzjMM0kjWJdw",
        "outputId": "a1d89b78-14ce-4b14-b438-9922ccd87f56"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The size of the vocabulary (number of distinct characters):  30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_text(text):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return tf.squeeze(vectorize_layer(text))"
      ],
      "metadata": {
        "id": "9tLF3jB2WVwm"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize the data.\n",
        "X_train_ds = X_train_ds_raw.map(vectorize_text)\n",
        "y_train_ds = y_train_ds_raw.map(vectorize_text)\n",
        "\n",
        "X_train_ds.element_spec, y_train_ds.element_spec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_TE-IxVWYKt",
        "outputId": "760e05a5-0a97-4d95-c4b7-873e90b2eeba"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(20,), dtype=tf.int64, name=None),\n",
              " TensorSpec(shape=(20,), dtype=tf.int64, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_ds=y_train_ds.map(lambda x: x[0])"
      ],
      "metadata": {
        "id": "Fcsq1BiqXaB_"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds =  tf.data.Dataset.zip((X_train_ds,y_train_ds))"
      ],
      "metadata": {
        "id": "IHn3MFSvXc38"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_ds = train_ds.shuffle(buffer_size=512).batch(batch_size, drop_remainder=True).cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "zdXOK80XXghp"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Result of the Data Pipeline:"
      ],
      "metadata": {
        "id": "DaLbUomuXpBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for sample in train_ds.take(1):\n",
        "  print(\"input (X) dimension: \", sample[0].numpy().shape, \"\\noutput (y) dimension: \",sample[1].numpy().shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26yDZTtrXoXd",
        "outputId": "f4728a13-87f8-469e-9bba-abf15f819f9f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input (X) dimension:  (64, 20) \n",
            "output (y) dimension:  (64,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sample in train_ds.take(1):\n",
        "  print(\"input (sequence of chars): \", sample[0][0].numpy(), \"\\noutput (next char to complete the input): \",sample[1][0].numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2gnVAy8X1Ub",
        "outputId": "0dc0913d-8cc5-4e59-955a-0e55f7da50be"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input (sequence of chars):  [ 9  3  2 12  5 18 10  2  5 10  2 18  6 11 24 10  2  6 17  0] \n",
            "output (next char to complete the input):  2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sample in train_ds.take(2):\n",
        "  print(\"input (sequence of chars): \", decode_sequence (sample[0][0].numpy()), \"\\noutput (next char to complete the input): \",vectorize_layer.get_vocabulary()[sample[1][0].numpy()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "id": "O_tp1XRqX3p1",
        "outputId": "d665e74c-802e-41e1-d596-90e98b4c3879"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-94a4d4855071>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"input (sequence of chars): \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_sequence\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\noutput (next char to complete the input): \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvectorize_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'decode_sequence' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PREPARE SAMPLING METHODS"
      ],
      "metadata": {
        "id": "PUGIJ01qX8g_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Text Generation, **sampling** means randomly **picking** the next token according to the generated **conditional probability distribution**.\n",
        "\n",
        "That is, after generating the conditional  probability distribution over the set of tokens (*vocabulary*) for the given input sequence, we need to  carefully decide how to **select the next token** (***sample***) from this distribution. \n",
        "\n",
        "\n",
        "\n",
        "There are **several methods for sampling** in text generation (see [here](https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277) and [here](https://huggingface.co/blog/how-to-generate)):\n",
        "\n",
        "\n",
        "* **Greedy Search (Maximization)** \n",
        "\n",
        "\n",
        "* **Temperature Sampling**\n",
        "\n",
        "* **Top-K Sampling**\n",
        "* **Top-P Sampling (Nucleus sampling)**\n",
        "\n",
        "* **Beam Search**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "o3o69VKbYapT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T93cen5DxM1j"
      },
      "source": [
        "def softmax(z):\n",
        "   return np.exp(z)/sum(np.exp(z))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_search(conditional_probability):\n",
        "  return (np.argmax(conditional_probability))"
      ],
      "metadata": {
        "id": "uCjzL5w_YQ_J"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def top_k_sampling(conditional_probability, k):\n",
        "  top_k_probabilities, top_k_indices= tf.math.top_k(conditional_probability, k=k, sorted=True)\n",
        "  top_k_probabilities= np.asarray(top_k_probabilities).astype(\"float32\")\n",
        "  top_k_probabilities= np.squeeze(top_k_probabilities)\n",
        "  top_k_indices = np.asarray(top_k_indices).astype(\"int32\")\n",
        "  top_k_redistributed_probability=softmax(top_k_probabilities)\n",
        "  top_k_redistributed_probability = np.asarray(top_k_redistributed_probability).astype(\"float32\")\n",
        "  sampled_token = np.random.choice(np.squeeze(top_k_indices), p=top_k_redistributed_probability)\n",
        "  return sampled_token"
      ],
      "metadata": {
        "id": "6AuwlkEbYw9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##A LSTM-BASED LANGUAGE MODEL FOR TEXT GENERATION"
      ],
      "metadata": {
        "id": "HO1JvOLXY0A-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " the length of the input (X) sequence (sequence_length) is 20 tokens (chars).\n",
        "\n",
        "Adding a layer to map those vocab indices into a space of dimensionality 'embedding_dim'.\n",
        "\n",
        "After applying Dropout, we use an LSTM layer to process the sequence and learn to generate the next token by the help of a Dense layer."
      ],
      "metadata": {
        "id": "b-cAYjd3Zyqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tf.keras.Input(shape=(sequence_length), dtype=\"int64\")\n",
        "x = layers.Embedding(max_features, embedding_dim)(inputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "x = layers.LSTM(128, return_sequences=True)(x)\n",
        "x = layers.Flatten()(x)\n",
        "predictions=  layers.Dense(max_features, activation='softmax')(x)\n",
        "model_LSTM = tf.keras.Model(inputs, predictions,name=\"model_LSTM\")"
      ],
      "metadata": {
        "id": "OXsrkzUPY1aN"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile the model"
      ],
      "metadata": {
        "id": "0VxyAZSzcMko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_LSTM.compile(loss='sparse_categorical_crossentropy', \n",
        "                   optimizer='adam', metrics=['accuracy'])\n",
        "print(model_LSTM.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtPfAgWVcOj6",
        "outputId": "688665cd-9218-4e0b-e7e0-f840148eff88"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_LSTM\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 20)]              0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 20, 16)            1536      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 20, 16)            0         \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 20, 128)           74240     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2560)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 96)                245856    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 321,632\n",
            "Trainable params: 321,632\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model"
      ],
      "metadata": {
        "id": "n1mHA2HsaOkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_LSTM.fit(train_ds, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_2RNl8AaPIO",
        "outputId": "54a19a3e-409b-4c22-acf7-8a83161d1823"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "9145/9145 [==============================] - 511s 55ms/step - loss: 2.3589 - accuracy: 0.3138\n",
            "Epoch 2/10\n",
            "9145/9145 [==============================] - 379s 41ms/step - loss: 2.0096 - accuracy: 0.4081\n",
            "Epoch 3/10\n",
            "9145/9145 [==============================] - 371s 41ms/step - loss: 1.8742 - accuracy: 0.4477\n",
            "Epoch 4/10\n",
            "9145/9145 [==============================] - 390s 43ms/step - loss: 1.7970 - accuracy: 0.4705\n",
            "Epoch 5/10\n",
            "9145/9145 [==============================] - 389s 43ms/step - loss: 1.7455 - accuracy: 0.4850\n",
            "Epoch 6/10\n",
            "9145/9145 [==============================] - 378s 41ms/step - loss: 1.7076 - accuracy: 0.4955\n",
            "Epoch 7/10\n",
            "9145/9145 [==============================] - 381s 42ms/step - loss: 1.6782 - accuracy: 0.5035\n",
            "Epoch 8/10\n",
            "9145/9145 [==============================] - 383s 42ms/step - loss: 1.6558 - accuracy: 0.5100\n",
            "Epoch 9/10\n",
            "9145/9145 [==============================] - 381s 42ms/step - loss: 1.6380 - accuracy: 0.5150\n",
            "Epoch 10/10\n",
            "9145/9145 [==============================] - 385s 42ms/step - loss: 1.6222 - accuracy: 0.5193\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe472487150>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below function to convert the given token index to the corresponding character for each token in the generated text"
      ],
      "metadata": {
        "id": "dI6sp7m8aXMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence (encoded_sequence):\n",
        "  deceoded_sequence=[]\n",
        "  for token in encoded_sequence:\n",
        "    deceoded_sequence.append(vectorize_layer.get_vocabulary()[token])\n",
        "  sequence= ''.join(deceoded_sequence)\n",
        "  print(\"\\t\",sequence)\n",
        "  return sequence\n"
      ],
      "metadata": {
        "id": "5Cn3vOsQaZ7_"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To generate text with various sampling methods, we prepared the following function. The generate_text(model, prompt, step) function takes the trained Language Model, the prompt, and the length of the text to be generated as the parameters. Then, it generates text with two different sampling methods."
      ],
      "metadata": {
        "id": "01jHP0EKaml-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, seed_original, step):\n",
        "    seed= vectorize_text(seed_original)\n",
        "    print(\"The prompt is\")\n",
        "    decode_sequence(seed.numpy().squeeze())\n",
        "    \n",
        "\n",
        "    seed= vectorize_text(seed_original).numpy().reshape(1,-1)\n",
        "    #Text Generated by Greedy Search Sampling\n",
        "    generated_greedy_search = (seed)\n",
        "    for i in range(step):\n",
        "      predictions=model.predict(seed)\n",
        "      next_index= greedy_search(predictions.squeeze())\n",
        "      generated_greedy_search = np.append(generated_greedy_search, next_index)\n",
        "      seed= generated_greedy_search[-sequence_length:].reshape(1,sequence_length)\n",
        "    print(\"Text Generated by Greedy Search Sampling:\")\n",
        "    decode_sequence(generated_greedy_search)\n",
        "\n",
        "    #Text Generated by Top-K Sampling\n",
        "    print(\"Text Generated by Top-K Sampling:\")\n",
        "    for k in [2, 3, 4, 5]:\n",
        "        print(\"\\tTop-k: \", k)\n",
        "        seed= vectorize_text(seed_original).numpy().reshape(1,-1)\n",
        "        generated_top_k = (seed)\n",
        "        for i in range(step):\n",
        "            predictions=model.predict(seed)\n",
        "            next_index = top_k_sampling(predictions.squeeze(), k)\n",
        "            generated_top_k = np.append(generated_top_k, next_index)\n",
        "            seed= generated_top_k[-sequence_length:].reshape(1,sequence_length)\n",
        "        decode_sequence(generated_top_k)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QO17YvkUatEM"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can call the generate_text() function by providing the trained LM, a prompt and the sequence length of the text to be generated as below."
      ],
      "metadata": {
        "id": "093d0dLwa4FS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can run this method for multiple times to observe the generated text with different sampling methods."
      ],
      "metadata": {
        "id": "NeYcw8n8a2ef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(model_LSTM,\"The Republic of Plato\", 20)"
      ],
      "metadata": {
        "id": "UuJIlfwIa9ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "4b27bc77-78be-4124-bbeb-b04b45c87789"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The prompt is\n",
            "\t the republic of plat\n",
            "Text Generated by Greedy Search Sampling:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-5a2b0ef96b38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_LSTM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"The Republic of Plato\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-33-39d69593d27d>\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(model, seed_original, step)\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mgenerated_greedy_search\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Text Generated by Greedy Search Sampling:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mdecode_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_greedy_search\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m#Text Generated by Top-K Sampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-c3dd4f015e3c>\u001b[0m in \u001b[0;36mdecode_sequence\u001b[0;34m(encoded_sequence)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mdeceoded_sequence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mencoded_sequence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdeceoded_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorize_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0msequence\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeceoded_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Mej1sm7Utlyi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}